\documentclass[pdflatex]{sn-jnl}
\jyear{2024}
\usepackage{multibib}
\newcites{m}{Methods References}
\usepackage[superscript]{cite}
\usepackage{caption}
\bibliographystyle{unsrt}
\bibliographystylem{unsrt} 
\raggedbottom

% Remove numbering from sections and subsections, as requested in decision email.
\setcounter{secnumdepth}{0}


\newcommand{\yohai}[1]{{\textcolor{red}{#1}}}
\newcommand{\neri}[1]{{\textcolor{cyan}{#1}}}


\begin{document}

\title[Running Title]{Does an earthquake “know” how big it will be? A neural-net aided study} % 2nd option: Using past seismicity to predict the magnitude of future earthquakes

\author[1,2]{\fnm{Neri} \sur{Berman}}\email{neriberman@gmail.com}
\author[2]{\fnm{Oleg} \sur{Zlydenko}}
\author[2]{\fnm{Oren} \sur{Gilon}}
\author[1]{\fnm{Yohai} \sur{Bar-Sinai}}\email{ybarsinai@gmail.com}

\affil[1]{\orgdiv{Department of Physics}, \orgname{Tel-Aviv University}, \orgaddress{\city{Tel-Aviv}, \country{Israel}}}
\affil[2]{\orgdiv{Google Research}, \orgname{Google}, \orgaddress{\city{Tel-Aviv}, \country{Israel}}}


\abstract{
Earthquake occurrence is notoriously difficult to predict. Salient features of the statistical properties of the timing and location of earthquakes, i.e. spatiotemporal clustering, can be relatively well captured by point-process models. However, very little is known regarding the magnitude of future events, and it is deeply debated whether it is possible to predict the magnitude of an earthquake before it starts, both due to the lack of information about the fault condition and to the inherent complexity of rupture dynamics. Consequently, even state of the art forecasting models typically assume no knowledge about the magnitude of future events, besides the time-independent Gutenberg Richter (GR) distribution, which describes the marginal distribution over wide regions and long times. This approach implicitly assumes that earthquake magnitudes are independent of seismic history and are identically distributed. In this work we challenge this view by showing that information about the magnitude of an upcoming earthquake can be directly extracted from the seismic history. We present a neural network-based model for probabilistic forecasting of future magnitudes based on cataloged properties: hypocentral locations, occurrence times and magnitudes of past earthquakes. Our history-dependent model outperforms stationary and quasi-stationary state of the art GR-based benchmarks, in real catalogs in Southern California, Japan and New-Zealand.  This demonstrates that earthquake catalogs contain information about the magnitude of future earthquakes, prior to their occurrence. We conclude by proposing methods to apply the model in characterization of the preparatory phase of earthquakes, and in operational hazard alert and earthquake forecasting systems.
}

\keywords{}

\maketitle

\neri{comment by Neri} \newline
\yohai{comment by Yohai}


\section{Introduction} \label{sec:introduction}



Earthquake forecasting is a long-standing scientific and technological challenge, often deemed unrealistic due to the inherent complexity of earthquake processes \cite{bernard_earthquake_1999, geller_earthquakes_1997}. Research since the late 19th century has provided much phenomenological insight about the spatiotemporal statistics of earthquakes. This includes various characteristics of spatial and temporal clustering \cite{omori_after-shocks_1894, kagan_short-term_2004, ben-zion_localization_2020}, which can be incorporated into stochastic prediction schemes \cite{ogata_statistical_1988, hardebeck_aftershock_2024, devries_deep_2018, king_static_1994}. These forecasting methods focus on the rate, and sometimes the location, of earthquake occurrence [cite]. Furthermore, these spatiotemporal properties are almost always treated independently from the magnitude, an assumption known as separability \cite{schoenberg_testing_2004}. That is, while seismic history is used to inform the location and rate of future earthquakes, i.e. by assuming some phenomenological description of spatiotemporal clustering, the magnitude of future events prediction often relies on drawing values from the Gutenberg-Richter (GR) distribution or its variants \cite{gutenberg_frequency_1944, kagan_seismic_2002, ogata_exploring_2018}. Importantly, this distribution either depends weakly or does not depend on the seismic history, implying that, conditioned on the fact that an earthquake occurs, its magnitude is taken to be independent of the rate and location of past events.

% This might be something to remove to shorten
% This modeling approach, implicitly assuming that faults hold no information about an earthquake magnitude before its occurrence, is not a naive or uninformed choice. On the contrary, it is supported by a broad range of physical models which describe earthquake statistics as a critical phenomenon (cite OFC, Bak's papers, Sornette 91, Sornette 2006, Wyart's new stuff). These models posit that faults evolve (``self-organize'') towards a critical state, where events emerge stochastically and their magnitude follows a power-law distribution which is scale-free and self-similar, akin to physical systems in the vicinity of a phase transition. Under this paradigm, determining the magnitude of an event requires a full microscopic knowledge of the system's state, due to the chaotic nature of rupture dynamics. In accord, point-process models predict (stochastically) the location and timing of earthquakes, but their magnitude is drawn from a constant or slowly evolving distribution (cite ETAS, other magnitude models). 

While assuming that an earthquake's magnitude is independent of the seismic history is popular due to its simplicity and its ease of use in earthquake modeling, it is not necessarily true. Indeed, the magnitude of an earthquake is a property of the rupture process, which is governed by the frictional properties of the fault and the stress field. These in turn depend on slip history in a deterministic manner, albeit intangibly complicated. Therefore, the assumption of magnitude separability, i.e. its independent from seismic history, is not a priory justified, and its validity should be tested empirically.
Though information about an earthquake magnitude may exists before its initiation, implying dependence on seismic history and non-separability, it has yet to be shown accessible using our current experimental or analytic tools. Current research on magnitude prediction has failed to show a universal and replicable advantage over the common benchmarks \cite{shcherbakov_forecasting_2019, ogata_exploring_2018, stockman_forecasting_2023,  panakkat_neural_2007}, by that leaving the question of accessible information prior to an earthquake's nucleation withstanding. 
\newline
\neri{perhaps a good place to mention Giuseppe-Zhuang work, Sam's  and Kelian's}
In this paper we explore if information about an earthquake's magnitude can be obtained from regional seismic history. This is done by constructing a neural-based model that predicts the magnitude of a given earthquake, given the short and long term seismic history prior to its occurrence. Importantly, our model is not tasked with predicting the timing and location of the event, as they are explicitly provided to the model. Thus, we separate out the task of predicting the nucleation of events and isolate the question of magnitude predictability. If our model performs better than a random draw from a GR distribution or its variants, as we will indeed demonstrate is the case, we assert that at least some information about the magnitude of an earthquake is extractable from cataloged properties alone. 


This contribution has two important consequences. From a fundamental point of view, it challenges the self-organized-criticality viewpoint of earthquake magnitude emergence, which posits that magnitudes are inherently unpredictable and do not depend on seismic history \cite{olami_self-organized_1992, sornette_self-organized_1989, bak_earthquakes_1989, de_geus_scaling_2022}. Second, it suggests that the separability assumption, which is widely applied in earthquake forecasting, may be replaced by a more nuanced model that incorporates the seismic history into the magnitude prediction. This may lead to improved forecasting models, and potentially also be used to identify precursory signals in the seismic history of an earthquake.

\neri{Is a literature review nescessary here or is it ingested enough in the text itself? I'm pretty satisfied with how the paragraphs up to now are written.}\newline
\neri{The entire paragraph going on about SOC, perhaps to long and should be inserted as a sentence somewhere in the literature review? can be seem as comment in source code.}
% This modeling approach, implicitly assuming that faults hold no information about an earthquake magnitude before its occurrence, is not a naive or uninformed choice. On the contrary, it is supported by a broad range of physical models which describe earthquake statistics as a critical phenomenon (cite OFC, Bak's papers, Sornette 91, Sornette 2006, Wyart's new stuff). These models posit that faults evolve (``self-organize'') towards a critical state, where events emerge stochastically and their magnitude follows a power-law distribution which is scale-free and self-similar, akin to physical systems in the vicinity of a phase transition. Under this paradigm, determining the magnitude of an event requires a full microscopic knowledge of the system's state, due to the chaotic nature of rupture dynamics. In accord, point-process models predict (stochastically) the location and timing of earthquakes, but their magnitude is drawn from a constant or slowly evolving distribution (cite ETAS, other magnitude models). 



In order to tackle the question of magnitude predictability, we construct the MAGnitude prediction Neural NETwork model, MAGNNET, a neural network (NN) for producing a probability density function (PDF) of the possible magnitudes at a queried space time coordinates, conditioned on the seismic history. MAGNNET encodes the seismic history of time series from a hypocentral catalog and produces an estimate of the possible magnitudes at a queried time and place.

MAGNNET's result will be quantitatively compared to other known benchmarks to estimate the amount of information gained by encoding the seismic history.
MAGNNET is composed of a long-short term memory (LSTM) architecture to model the seismic history up to a given time. This latent representation is then passed together with  a space-time coordinate query of a future earthquake into a fully connected neural network (FCNN) that produces a parametrized PDF of the possible magnitudes at the quarried space- time coordinates. The task described here, from encoding the seismic history to producing a PDF for a specific time-space querry is depicted in Fig. \ref{fig:intro_fig}a. A detailed description of the model's architecture is given in the methods section \neri{TODO}. We optimize MAGNNET's parameters to maximize the log likelihood (LL) of the true magnitude label, by using the loss function $\mathcal{L}$:
\begin{equation}
    \mathcal{L} = -\langle \log{ \left( p_{\textbf{x}_i, t_i} \left( m_i \right) \right) } \rangle\ ,
    \label{eq:loss_function}
\end{equation}
where $p_{\textbf{x}_i, t_i}$ is the probability function returned by the model for the space-time coordinates $\textbf{x}_i, t_i$ (i.e. the ``query coordinates''), $m_i$ the corresponding to the actual magnitude of the $i^{th}$ earthquake and $\langle \cdot\rangle$ stands for the empirical average over the data set. We use the a mixture of two Kumaraswamy distributions \cite{kumaraswamy_generalized_1980} parameterized family of probability distributions stretched to a relevant domain. The final distribution used is defined by 5 parameters. This family smoothly can interpolate between a decaying distribution (resembling the GR distribution), and localized distributions whose mass is concentrated around a specific value. This allows the model to output both an ``ignorant'' prediction, essentially resembling the GR distribution, and more confident predictions localized around a given magnitude.

We train MAGNNET on three distinct earthquake catalogs to assess the performance across diverse seismogenic regions: the Hauksson Catalog [cite] for Southern California, GeoNet [cite] for New Zealand, and the JMA catalog [cite] for Japan. While all three catalogs encompass highly active seismic zones, they are compiled using various measurement methodologies (e.g.? cite?) and exhibit varying data quality. A separate model is trained for each region, with identical loss function and parameterization of the PDF. Sample results of known events worldwide are presented in \ref{fig:intro_fig}b. The PDF of each event is shown superimposed with the train set's Gutenberg-Richter distribution, i.e. the common vanilla benchmark.


\begin{figure}[h!]
	\centering
        \includegraphics[width=1\textwidth]{figures/intro_fig.pdf}
	\caption{
 Caption.
}
\label{fig:intro_fig}
\end{figure}


The model's, MAGNNET's, performance is evaluated over a time span that was not included in training. The presented metrics demonstrate that our model consistently and significantly outperforms all benchmarks across all test regions, indicating an information gain in forecasting earthquake magnitudes prior to their occurrence. \neri{I think this is a good place to add that we conclude that there is some preparatory phase.}
%This finding directly suggests that the seismic system undergoes preparatory processes tailored to the impending event's magnitude before its initiation. Examining these results across multiple catalogs and regions highlights the robustness of our methodology and the generalizability of our conclusion, paving the way for further exploration of earthquake predictability.



\section{Results} \label{sec:results}

Focusing on existing events within the catalog due to our model's requirement for labeled data, our analysis utilizes the space-time coordinates of earthquakes as input. This necessitates excluding queries about arbitrary spatiotemporal points during training and evaluation, as these processes require knowledge of the true magnitude for each event at its specific location and time. Consequently, the raw output for our analysis consists of the model-generated PDFs of possible magnitudes for each event's time and location. Figure \ref{fig:model_output}a \neri{TODO: make sure numbering in this figure is correct} showcases PDFs produced by our model, $p_{\textbf{x}_i, t_i}(m)$, for 100 randomly sampled event times and locations from the test set of Southern California. Included as a baseline is the stationary GR predictor trained on the training set, representing its potential operational use (e.g., Tsuruoka et al., 2012). A clear trend emerges: PDFs predicted for higher-magnitude events (warm colors) exhibit a greater skew towards higher magnitudes compared to those for lower-magnitude events (cooler colors). Figure \ref{fig:model_output}b Presents the distribution of lieklihood scores of the test set. That is, $p_{\textbf{x}_i, t_i}^{(model)}(magntude label)$, the probability density for retrieving the label magnitude of an event from a given model. The solid line indicates the median, percentiles 10 and 90 are indicated by the shaded region. Here it can be seen that \neri{rerun this figure before writing conclusions}. To qualitativlry compare our model to other benchmark model in more common terms, it is useful to examine the marginal distribution for the magnitudes of the test set. \ref{fig:model_output}c presents the marginal PDF, $p(m)$, of MAGNNET compared to other common benchmarks for the Southern California dataset. This depicts the general trend that the model suggests and makes the comparison to common benchmarks, such as the GR, closer to the method they are typically used in, as a stationary distribution. \ref{fig:model_output}c also shows the histograms of the train and test set superimposed with the marginal PDFs of the models examined. It can clearly be seen that GR follows the train set, as expected, while MAGNNET follows the test set. Furthermore, the dynamic GR benchmarks such as last $n$ events of the $d$ days, although showing a closer functional for to the test than the train, lack features of the histogram that MAGNNET does not. 


\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/raw_results_hauksson.pdf}
    \caption{
        \textbf{outputs of MAGnitude prediction Neural NETwork, MAGNNET, for the Hauksson catalog of Southern California. a}, Likelihoods of MAGNNET and some common benchmark magnitude predictors. Solid lines represent the median value along the magnitude axis, shading shows the 10th and 90th percentiles of the distribution along the vertical axis. \textbf{b}, Train set's Gutenberg-Richter dostribution (dashen black line) with PDFs produced by MAGNNET for 100 randomly sampled events from the test set. Each PDF is color coded by the label of the event, its true magnitude. Colorbar to the right indicates the color code. \textbf{c}, Total magnitude frequency distribution produced by MAGNNET and common benchmarks for the entire test set. Histograms of the train (and test) set are presented in orange (blue).
    }
    \label{fig:model_output}
\end{figure}


Quantification of a model's preformance in our scheme will be done by averging on all log-likelihood scores of the test set, as defined in Eq. \ref{eq:loss_function}. Fig. \ref{fig:metrics}a-c shows the minus mean log likelihood score, $\mathcal{L}$ from Eq. \ref{eq:loss_function}, for MAGNNET and chosen common benchmarks, for the three regions tested. It can be clearly seen that MAGNNET shows a clear an significant average information gain over every benchmark tested. \neri{Should we mention other benchmarks and refer to the SM?}. 



\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/combined_batplots.pdf}
    \caption{
        \textbf{Metric scores for out model MAGNNET, and common benchmark models. a}, \textbf{b}, \textbf{c}, Minus mean information content of our MAGNNET model (red) and other common benchmark magnitude predictors (see labels on bars in the figure). \textbf{d, e, f}, Reciever Operating Characteristic (ROC) curve for a binary classifier determinng the existence of a large ($m>=4$) event. Area under the curve (AUC) for each curve is written in each frame, colorcoded identically to the bar plots. \textbf{g, h, i}, Precision at recall curves for the binary classifier described in d, e, f. 
        }
        \label{fig:metrics}
\end{figure}

As the out model's output is a probability density function (PDF), we may easily convert it to a binary classifer. By doing this we are posing a modified question: will there be a \textit{large} earthquake in the query space-time coordiantes or not? This is done by setting a threshold which defines a "\textit{large}" earthquake, $m_c$ ($=4$ in the examples presented here), then setting examples above that threshold as positive examples and below as negative. The probability of a large event occuring in the query coordinates is given by integrating the output PDF above the threshild, $p=\int_{m_c}^{\infty}p(m')dm'$. Given such binary classifier, it is possible now to examine the common binary classifier metrics of area under reciever operator (ROC) curve [cite] and the prescision at recall curve [cite]. \ref{fig:metrics}d-f show the ROC curves for the binary modification for all three regions in question. The area under the curve (AUC) are written to the bottom right of each panel color coded by the same legend as the bar plots above. It can be seen that throughout all regions MAGNNET preforms significantly better than all other benchmarks, indicated by the largest AUC value. The precision at recall curve displayed in \ref{fig:metrics}g-i show a similar result as MAGNNET's curve displays the largest AUC among all benchmarks tested throughout all regions.
\neri{TODO: calculate and present AUC P@R}

   
    
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/info_over_time_hauksson.pdf}
    \caption{
        Caption.
    }
    \label{fig:info_gain_over_time}
\end{figure}
    


The information gain achieved by our model for each event can be quantified by the difference in log-likelihood (LL) between our model's prediction and the baseline GR predictor:
\begin{equation}
    \Delta LL = \log{p_{\pmb{x_i}, t_i}^{(MAGNNET)}(m_i)} - \log{p^{(GR)}(m_i)}
    \label{eq:information_gain}
\end{equation}
This equation measures the amount of information gained by using our MAGNNET model compared to the GR benchmark. Analyzing this quantity per event across the entire test set reveals patterns in where our model achieves its advantage. Fig. \ref{fig:info_gain_over_time}a shows the test set for Southern California event magnitudes by their running index, colored according to the information gain of our model over the GR benchmark. This time-domain representation allows us to track the cumulative information gain, calculated over the entire test set and displayed as the black dashed line in the figure. The predominantly increasing trend in cumulative information gain suggests that our model is consistently advantageous as opposed to a concentrated advantage in specific periods. Notably, for the Southern California data set, the most rapid information gain occurs during the aftershocks of the two Ridgecrest earthquakes. Surprisingly, even the pre-shock and main shock events demonstrate positive information gains over the GR benchmark, with values of $0.5$ and $1.4, \ \Delta LL$, respectively, albeit these events are typically considered completely unpredictable. This observation leads to a significant corollary: at least some portion of the information gain likely originates from a statistical signal embedded in the event sequence prior to event's occurence. This point will be further elaborated upon in the discussion section.
\newline
The observed increase in information gain persists throughout the entire test set for all regions examined in this study as the black dashed curves in Fig. \ref{fig:info_gain_over_time}b-d present. This includes regions previously analyzed by (Ogata et al. SRL 2018), where no information gain was detected using the method employed at the time. Further analysis of \ref{fig:info_gain_over_time} applied to other regions is presented in the supplementary material. \neri{TODO: ad these colorful plots for NZ and JAPAN to the SM}
The smoothed instantaneous information gain per event is presented in Fig. \ref{fig:info_gain_over_time}e-g, and can be seen as commonly well above the threshold of zero information gain, marked by the red dashed line.
\newline
While the information gain observed in preshock and mainshocks presented above is encouraging, a potential concern might arise regarding the aftershocks: could their information gain be an artifact of fitting to the temporal incompleteness of the catalog (cite somethong?)? To address this, we factor out this potential bias by recalculating the likelihood score of each event, $p(m_i)$ , conditioned on the magnitude exceeding a threshold, $\Tilde{m}$:

\begin{equation}
    p \left( m \vert m_i > \Tilde{m} \right) = \frac{p_{\pmb{x}_i, t_i}(m)} {\int_{\Tilde{m}}^{\infty} p(m') ,dm'}
    \label{eq:conditioned_likelihood}
\end{equation}

For our analysis, we set $\Tilde{m} = m_c(t)$, determined dynamically using the maximum curvature method (Weimer and Weiss, 2000) within a window of 150 past and 150 future events. The resulting temporal incompleteness curves are presented in the SM \neri{TODO: add the $m_c(t)$ figure to SM}. After applying the renormalization of Eq. \ref{eq:conditioned_likelihood} the cumulative information gain is recalculated. The result is presented as the grey dashed curved in Fig.  \ref{fig:info_gain_over_time}a, as well as in Fig. \ref{fig:info_gain_over_time}b-d for all three tested regions.
\newline
A second concern regarding temporal variations of the completeness magnitude may be raised to oppose to the analysis and results presented in this work. The following paragraph will provide an explanation of our method for varifying that we are indeed measuring a true statistical trend that is not created by the local completeness magnitude artifact. We start by deviding each region into bins of size [dx, dy] \neri{TODO fill in the details}. The average log likelihood score $\mathcal{L}$ is calculated per bin for both MAGNNET and GR, resulting in a map of spatial average log likelihood, per model. The subtraction of the two maps is presented in Fig. \ref{fig:info_gain_over_time}h-j, showing the spatial distribution of information gain for each region. 
\neri{TODO complete paragraph about how I select only bins of mc(x,y)<=mc to recalculate the average score and show that I still gain information.}
for which the local completness magnitude $m_c(x, y)$ is calculated. 





% There are two bibliographies in Nature papers. One for the main text, and one for the Methods and Extended Data sections. The numbering is sequential, meaning that the reference section for the Methods and Extended Data section starts after the last number from the main reference section. References should not appear in both sections. Any reference used in Methods or Extended Data that also appears in the main bibliography should *only* appear in the main bibliography.
\let\oldbibliography\thebibliography
\renewcommand{\thebibliography}[1]{%
  \oldbibliography{#1}%
  \setlength{\itemsep}{10pt}%
}
% \bibliography{bibliography}
% \bibliography{bib-article.bib}
\newpage
\bibliography{Magnitude_prediction_paper}

\let\oldthebibliography=\thebibliography
\let\oldendthebibliography=\endthebibliography
\renewenvironment{thebibliography}[1]{
    \oldthebibliography{#1}
    % The number here (34 is an example) is the number of references in the main bibliography, and thus defines the starting number of the first reference in the Methods and Extended Data bibliography.
    \setcounter{enumiv}{34}
}{\oldendthebibliography}

% Figure legends appear after the text, not placed in the text. Do not include the actual image files in the article. Images are submitted as separate files. During the first submission to the journal, you can include the images in the article file for readability, but if you pass the reviews, they will want the images removed from the main article.
\newpage
\unnumbered

\unnumbered
\section{Methods}
\subsection{AI Model}
\textbf{Grey's Note} \textit{My understanding is that you can write the methods section just how you would a normal methods section for a normal scientific article.}


\section*{Data Availability}
This is a required section. Guidelines for data availability: \url{https://www.nature.com/documents/nr-data-availability-statements-data-citations.pdf}.

\section*{Code Availability}
\textbf{Grey's Note} \textit{This is a required section. If your article is about AI or ML, the editor will ask you to make the weights of a trained model available.}


\newpage
\renewcommand\refname{Methods References}
\begin{thebibliography}{10}

\bibitem{kratzert2019towards}
Frederik Kratzert, Daniel Klotz, Guy Shalev, G{\"u}nter Klambauer, Sepp
  Hochreiter, and Grey~S Nearing.
\newblock Towards learning universal, regional, and local hydrological
  behaviors via machine learning applied to large-sample datasets.
\newblock {\em Hydrology and Earth System Sciences}, 23(12):5089--5110, 2019.

\bibitem{klotz2022uncertainty}
Daniel Klotz, Frederik Kratzert, Martin Gauch, Alden~K Sampson, Johannes
  Brandstetter, G{\"u}nter Klambauer, Sepp Hochreiter, and Grey~S Nearing.
\newblock Uncertainty estimation with deep learning for rainfall--runoff
  modeling.
\newblock {\em Hydrology and Earth System Sciences}, 26(6):1673--1693, 2022.

\bibitem{twb2023gdp}
IBRD-IDA.
\newblock {The World Bank Data: Current US\$}.
\newblock https://data.worldbank.org/indicator/NY.GDP.MKTP.CD, 2023.
\newblock Accessed: 2023-06-04.

\end{thebibliography}


\newpage
\section*{Acknowledgements}
Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, inessential words, or effusive comments. A person can be thanked for assistance, not “excellent” assistance, or for comments, not “insightful” comments, for example. Acknowledgements can contain grant and contribution numbers.

\section*{Author Contributions}
Author Contributions: Authors are required to include a statement to specify the contributions of each co-author. The statement can be up to several sentences long, describing the tasks of individual authors referred to by their initials. See the authorship policy page for further explanation and examples.


\section*{Author Information}
\textbf{Grey's Note} \textit{Two things are required in this Author Information section: (1) A statement about competing interests. I have no advice about what constitutes a competing interest, you will have to read about it and make your own decision. (2) A clear statement about who to contact with question about the paper. An example is below.}

The authors declare no competing interests. Please contact either the first author (Grey Nearing; \href{mailto:nearing@google.com}{nearing@google.com}) or Avinatan Hassidim; \href{mailto:avinatan@google.com}{avinatan@google.com}) for correspondence and requests, including questions regarding reprints and permissions.


\newpage
\section*{Extended Data}
\textbf{Grey's Note} \textit{You get up to 10 items in the Extended Data section. All Tables and Figures should be enumerated as "Extended Data Figure 1", "Extended Data Table 1", etc. These must also be referenced the same way in the text. Extended Data tables and figures may be referenced in the main article and/or in the Methods section. I am doing the in-text references to these ED items manually, instead of trying to override the table numbering schemed in Latex. Please update this template to automate that process if you feel like doing that.}

\textit{You can use Latex formatting for tables, but the journal will eventually require that your tables be submitted as images. Yes, this also struck me as unusual.}

\textit{Each ED figure should be on a separate page.}

\textit{Finally, this note is here in the ED section, but in your paper, the ED section should not have any text except figure and table captions/legends.}

\end{document}
